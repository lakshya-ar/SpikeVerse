{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import loadmat\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation of Data for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = loadmat(\"MNIST/mnist-original.mat\")\n",
    "mnist_data = mnist[\"data\"].T\n",
    "mnist_label = mnist[\"label\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def poisson_spike_encoding(images, time_steps=100):\n",
    "    return (np.random.rand(images.shape[0], time_steps, 784) < images[:, None, :]).astype(np.uint8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose 3 indices\n",
    "indices = [0, 7000, 14000, 21000, 28000, 35000, 66987, 42000, 49000, 56000]\n",
    "images = mnist_data[indices]/255.0  # shape: (3, 784)\n",
    "labels = mnist_label[indices]  # shape: (3,)\n",
    "\n",
    "spike_trains = poisson_spike_encoding(images, time_steps=100)  # shape: (3, 100, 784)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(6, 30))\n",
    "\n",
    "for i in range(10):\n",
    "    # Digit image\n",
    "    plt.subplot(10, 2, 2*i + 1)\n",
    "    plt.imshow(images[i].reshape(28, 28), cmap=\"gray\")\n",
    "    plt.title(f\"Digit: {int(labels[i])}\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    # Spike train\n",
    "    plt.subplot(10, 2, 2*i + 2)\n",
    "    plt.imshow(spike_trains[i], cmap=\"Greys\", aspect=\"auto\")\n",
    "    plt.title(\"Poisson Spike Train\")\n",
    "    plt.xlabel(\"Pixel Index\")\n",
    "    plt.ylabel(\"Time Step\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"spike_mnist_dataset\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "time_steps = 100\n",
    "chunk_size = 1000\n",
    "num_samples = mnist_data.shape[0]  # 70,000\n",
    "input_dim = 784\n",
    "\n",
    "# Preallocate space on disk\n",
    "spike_path = os.path.join(output_dir, f\"spike_trains_{time_steps}ts.npy\")\n",
    "label_path = os.path.join(output_dir, \"labels.npy\")\n",
    "\n",
    "spike_memmap = np.memmap(spike_path, dtype=np.uint8, mode='w+', shape=(num_samples, time_steps, input_dim))\n",
    "label_memmap = np.memmap(label_path, dtype=np.uint8, mode='w+', shape=(num_samples,))\n",
    "\n",
    "print(\"Encoding and saving...\")\n",
    "\n",
    "for i in tqdm(range(0, num_samples, chunk_size)):\n",
    "    chunk = mnist_data[i:i+chunk_size] / 255.0\n",
    "    label_chunk = mnist_label[i:i+chunk_size]\n",
    "\n",
    "    spike_chunk = poisson_spike_encoding(chunk, time_steps).astype(np.uint8)\n",
    "\n",
    "    spike_memmap[i:i+chunk_size] = spike_chunk\n",
    "    label_memmap[i:i+chunk_size] = label_chunk\n",
    "\n",
    "# Flush data to disk\n",
    "spike_memmap.flush()\n",
    "label_memmap.flush()\n",
    "\n",
    "print(\"All spike data encoded and saved!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class SpikeMNISTDataset(Dataset):\n",
    "    def __init__(self, spike_file, label_file):\n",
    "        self.spikes = np.memmap(spike_file, dtype=np.uint8, mode=\"r\", shape=(70000, 100, 784))\n",
    "        self.labels = np.memmap(label_file, dtype=np.uint8, mode=\"r\", shape=(70000,))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        spike_tensor = torch.tensor(self.spikes[idx], dtype=torch.float32)\n",
    "        label_tensor = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return spike_tensor, label_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "dataset = SpikeMNISTDataset(\"spike_mnist_dataset/spike_trains_100ts.npy\",\n",
    "                             \"spike_mnist_dataset/labels.npy\")\n",
    "\n",
    "# Set lengths\n",
    "train_len = 60000\n",
    "test_len = len(dataset) - train_len\n",
    "\n",
    "\n",
    "generator = torch.Generator().manual_seed(42)\n",
    "\n",
    "\n",
    "train_dataset, test_dataset = random_split(dataset, [train_len, test_len], generator=generator)\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making of Neuron Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SurrogateSpike(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input, threshold):\n",
    "        ctx.save_for_backward(input - threshold)\n",
    "        return (input >= threshold).float()\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        (x,) = ctx.saved_tensors\n",
    "        beta = 10.0\n",
    "        grad = beta * torch.exp(-beta * x.abs()) / ((1 + torch.exp(-beta * x.abs())) ** 2)\n",
    "        return grad_output * grad, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spike_fn(input, threshold=1.0):\n",
    "    return SurrogateSpike.apply(input, threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LIFNeuron(nn.Module):\n",
    "    def __init__(self, size, threshold=1.0, tau=2.0):\n",
    "        super().__init__()\n",
    "        self.V = None  # initialized later\n",
    "        self.threshold = threshold\n",
    "        self.alpha = torch.exp(torch.tensor(-1.0 / tau))\n",
    "        self.size = size\n",
    "\n",
    "    def reset(self):\n",
    "        self.V = None\n",
    "\n",
    "    def forward(self, input):\n",
    "        if self.V is None or self.V.shape != input.shape:\n",
    "            self.V = torch.zeros_like(input)\n",
    "\n",
    "        # Leaky integration\n",
    "        self.V = self.alpha * self.V + input\n",
    "\n",
    "        # Spike generation using surrogate\n",
    "        S = spike_fn(self.V, self.threshold)\n",
    "\n",
    "\n",
    "        self.V = self.V * (1 - S)\n",
    "\n",
    "        return S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SNNLayer(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, threshold=1.0, tau=2.0):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(in_dim, out_dim, bias=False)\n",
    "        self.neuron = LIFNeuron(size=out_dim, threshold=threshold, tau=tau)\n",
    "\n",
    "    def forward(self, input_spikes):\n",
    "        I = self.linear(input_spikes)\n",
    "        return self.neuron(I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SNNModel(nn.Module):\n",
    "    def __init__(self, input_size=784, hidden_size=256, output_size=10, time_steps=100):\n",
    "        super().__init__()\n",
    "        self.time_steps = time_steps\n",
    "        self.layer1 = SNNLayer(input_size, hidden_size)\n",
    "        self.layer2 = SNNLayer(hidden_size, output_size)\n",
    "\n",
    "    def reset(self):\n",
    "        self.layer1.neuron.reset()\n",
    "        self.layer2.neuron.reset()\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        output_spike_count = torch.zeros((batch_size, 10), device=x.device)\n",
    "\n",
    "        for t in range(self.time_steps):\n",
    "            input_t = x[:, t, :]\n",
    "            spikes_hidden = self.layer1(input_t)\n",
    "            spikes_out = self.layer2(spikes_hidden)\n",
    "            output_spike_count += spikes_out\n",
    "\n",
    "        return output_spike_count\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SNNModel(time_steps=100).to(\"cuda\")\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "def train_step(batch_spikes, batch_labels):\n",
    "    batch_spikes = batch_spikes.to(\"cuda\").float()\n",
    "    batch_labels = batch_labels.to(\"cuda\")\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    model.reset()  # ← ensures no memory leak in neuron state\n",
    "    out = model(batch_spikes)\n",
    "    loss = criterion(out, batch_labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "    preds = out.argmax(dim=1)\n",
    "    correct = (preds == batch_labels).sum().item()\n",
    "    total_correct = correct\n",
    "    total_samples = batch_labels.size(0)\n",
    "\n",
    "    return loss.item(), total_correct, total_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "def train_step(batch_spikes, batch_labels):\n",
    "    batch_spikes = batch_spikes.to(\"cuda\").float()\n",
    "    batch_labels = batch_labels.to(\"cuda\")\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    model.reset()  # ← ensures no memory leak in neuron state\n",
    "    out = model(batch_spikes)\n",
    "    loss = criterion(out, batch_labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "    preds = out.argmax(dim=1)\n",
    "    correct = (preds == batch_labels).sum().item()\n",
    "    total_correct = correct\n",
    "    total_samples = batch_labels.size(0)\n",
    "\n",
    "    return loss.item(), total_correct, total_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs=10\n",
    "\n",
    "batch_losses = []\n",
    "batch_accuracies = []\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    for batch_spikes, batch_labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n",
    "        loss, correct, samples = train_step(batch_spikes, batch_labels)\n",
    "        total_loss += loss\n",
    "        total_correct += correct\n",
    "        total_samples += samples\n",
    "        batch_losses.append(loss)\n",
    "        batch_accuracies.append(correct*100/ samples)\n",
    "\n",
    "    acc = total_correct / total_samples * 100\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "\n",
    "\n",
    "    print(f\"Epoch {epoch+1} | Loss: {avg_loss:.4f} | Accuracy: {acc:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches = list(range(1,len(batch_losses)+1))\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(batches, batch_losses)\n",
    "plt.title(\"Training Loss\")\n",
    "plt.xlabel(\"Batchs\")\n",
    "plt.ylabel(\"Loss per Batch\")\n",
    "plt.grid(True)\n",
    "\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(batches, batch_accuracies, color='green')\n",
    "plt.title(\"Training Accuracy\")\n",
    "plt.xlabel(\"Batchs\")\n",
    "plt.ylabel(\"Accuracy (%) per Batch\")\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"snn_training_results.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    size=len(test_loader)\n",
    "\n",
    "    for i, (spikes, labels) in tqdm(enumerate(test_loader),total=len(test_loader)):\n",
    "    \n",
    "        spikes = spikes.to(\"cuda\").float()\n",
    "        labels = labels.to(\"cuda\")\n",
    "\n",
    "        out = model(spikes)\n",
    "        preds = out.argmax(dim=1)\n",
    "\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        total_correct += (preds == labels).sum().item()\n",
    "        total_samples += labels.size(0)\n",
    "\n",
    "    acc = total_correct / total_samples * 100\n",
    "    print(f\"\\nFinal Test Accuracy: {acc:.2f}% on {total_samples} samples\")\n",
    "\n",
    "\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=np.arange(10))\n",
    "    disp.plot(cmap=\"Blues\", values_format=\"d\")\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.grid(False)\n",
    "    plt.savefig(\"Confusion_Matrix.png\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "evaluate_model(model, test_loader)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
